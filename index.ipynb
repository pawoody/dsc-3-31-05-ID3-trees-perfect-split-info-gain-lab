{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ID3-Classification-Trees:-Perfect-Split-with-Information-Gain---Lab\" data-toc-modified-id=\"ID3-Classification-Trees:-Perfect-Split-with-Information-Gain---Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ID3 Classification Trees: Perfect Split with Information Gain - Lab</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Objectives\" data-toc-modified-id=\"Objectives-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Objectives</a></span></li><li><span><a href=\"#Problem\" data-toc-modified-id=\"Problem-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Problem</a></span></li><li><span><a href=\"#Write-a-function-entropy(pi)-to-calculate-total-entropy-in-a-given-discrete-probability-distribution-pi\" data-toc-modified-id=\"Write-a-function-entropy(pi)-to-calculate-total-entropy-in-a-given-discrete-probability-distribution-pi-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Write a function <code>entropy(pi)</code> to calculate total entropy in a given discrete probability distribution <code>pi</code></a></span></li><li><span><a href=\"#Write-a-function-IG(D,a)-to-calculate-the-information-gain\" data-toc-modified-id=\"Write-a-function-IG(D,a)-to-calculate-the-information-gain-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Write a function <code>IG(D,a)</code> to calculate the information gain</a></span></li><li><span><a href=\"#First-Iteration---Decide-Best-Split-for-master-node\" data-toc-modified-id=\"First-Iteration---Decide-Best-Split-for-master-node-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>First Iteration - Decide Best Split for master node</a></span></li><li><span><a href=\"#Second-Iteration\" data-toc-modified-id=\"Second-Iteration-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Second Iteration</a></span></li><li><span><a href=\"#Third-Iteration\" data-toc-modified-id=\"Third-Iteration-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Third Iteration</a></span></li><li><span><a href=\"#All-the-Other-Iterations\" data-toc-modified-id=\"All-the-Other-Iterations-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>All the Other Iterations</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-1.10\"><span class=\"toc-item-num\">1.10&nbsp;&nbsp;</span>Summary</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3 Classification Trees: Perfect Split with Information Gain - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we will simulate the example from the previous lesson in python. We will write functions to calculate entropy and IG which will be used for calculating these uncertainty measures and deciding upon creating a split using information gain while growing a ID3 classification tree. We shall attempt to write general function that can be used for other (larger) problems as well. So let's get on with it.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "- Write functions for calculating Entropy and Information gain measures\n",
    "- Identify the attribute for best split at master and each subsequent node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We shall use the same problem about deciding weather to go and play tennis on a given day, given the weather conditions. Here is the data from previous lesson:\n",
    "\n",
    "|  outlook | temp | humidity | windy | play |\n",
    "|:--------:|:----:|:--------:|:-----:|:----:|\n",
    "| overcast | cool |   high   |   Y   |  yes |\n",
    "| overcast | mild |  normal  |   N   |  yes |\n",
    "|   sunny  | cool |  normal  |   N   |  yes |\n",
    "| overcast |  hot |   high   |   Y   |  no  |\n",
    "|   sunny  |  hot |  normal  |   Y   |  yes |\n",
    "|   rain   | mild |   high   |   N   |  no  |\n",
    "|   rain   | cool |  normal  |   N   |  no  |\n",
    "|   sunny  | mild |   high   |   N   |  yes |\n",
    "|   sunny  | cool |  normal  |   Y   |  yes |\n",
    "|   sunny  | mild |  normal  |   Y   |  yes |\n",
    "| overcast | cool |   high   |   N   |  yes |\n",
    "|   rain   | cool |   high   |   Y   |  no  |\n",
    "|   sunny  |  hot |  normal  |   Y   |  no  |\n",
    "|   sunny  | mild |   high   |   N   |  yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function `entropy(pi)` to calculate total entropy in a given discrete probability distribution `pi`\n",
    "\n",
    "- The function should input a probability distribution `pi` as an array of class distributions\n",
    "- Calculate and return entropy according to the formula: $$Entropy(p) = -\\sum (P_i . log_2(P_i))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.0\n",
      "0.6500224216483541\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "def entropy(pi):\n",
    "    '''\n",
    "    return the Entropy of a probability distribution:\n",
    "    entropy(p) = - SUM (Pi * log(Pi) )\n",
    "    '''\n",
    "    \n",
    "    total = 0\n",
    "    for p in pi:\n",
    "        p = p/sum(pi)\n",
    "        if p != 0:\n",
    "            total += p*log(p,2)\n",
    "        else:\n",
    "            total += 0\n",
    "    total *= -1\n",
    "    return total\n",
    "\n",
    "# Test the function \n",
    "\n",
    "print(entropy([1,1])) # Maximum Entropy e.g. a coin toss\n",
    "print (entropy([0,6])) # No entropy, ignore the -ve with zero , its there due to log function\n",
    "print (entropy([2,10])) # A random mix of classes\n",
    "\n",
    "# 1.0\n",
    "# 0.0\n",
    "# 0.6500224216483541"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "-0.0\n",
      "0.6500224216483541\n"
     ]
    }
   ],
   "source": [
    "from math import log\n",
    "def entropy(pi):\n",
    "    #return the Entropy of a probability distribution:\n",
    "    total = 0\n",
    "    for p in pi:\n",
    "        p = p / sum(pi)\n",
    "        if p != 0:\n",
    "            total +=  p * log(p, 2)\n",
    "        else:\n",
    "            total += 0\n",
    "    total *= -1\n",
    "    return total\n",
    "\n",
    "# Test the function \n",
    "print(entropy([1,1])) # Maximum Entropy e.g. a coin toss\n",
    "print (entropy([0,6])) # No entropy, ignore the -ve with zero , its there due to log function\n",
    "print (entropy([2,10])) # A random mix of classes\n",
    "\n",
    "# 1.0\n",
    "# 0.0\n",
    "# 0.6500224216483541"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a function `IG(D,a)` to calculate the information gain \n",
    "\n",
    "- The function should input `D` as a class distribution array for target class, and `a` the class distribution of the attribute to be tested\n",
    "- Using the `entropy()` function above, calculate the information gain as:\n",
    "\n",
    "$$gain(D,A) = Entropy(D) - \\sum(\\frac{|D_i|}{|D|}.Entropy(D_i))$$\n",
    "\n",
    "where `Di` represents distribution of each class in `a`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5408520829727552\n"
     ]
    }
   ],
   "source": [
    "def IG(D, a):\n",
    "    '''\n",
    "    return the information gain:\n",
    "    gain(D, A) = entropy(D)âˆ’ SUM( |Di| / |D| * entropy(Di) )\n",
    "    '''\n",
    "    total=0\n",
    "    for Di in a:\n",
    "        total += abs(sum(Di)/sum(D)) * entropy(Di)\n",
    "    gain = entropy(D)-total\n",
    "    return gain\n",
    "\n",
    "\n",
    "# Uncomment to run the test\n",
    "\n",
    "# set of example of the dataset - distribution of classes\n",
    "test_dist = [6, 6] # Yes, No\n",
    "# attribute, number of members (feature)\n",
    "test_attr = [ [4,0], [2,4], [0,2] ] # class1, class2, class3 of attr1 according to YES/NO classes in test_dist\n",
    "\n",
    "print(IG(test_dist, test_attr))\n",
    "\n",
    "# 0.5408520829727552"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Iteration - Decide Best Split for master node\n",
    "\n",
    "- Create The class distribution `play` as a list showing frequencies of both classes from the dataset\n",
    "- Similarly create variables for four categorical feature attributes showing the class distribution for each class with respect to the target classes (yes and no)\n",
    "- Pass the play distribution with each attribute to calculate the information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:\n",
      "\n",
      "Outlook: 0.41265581953400066\n",
      "Temperature: 0.09212146003297261\n",
      "Humidity: 0.0161116063701896\n",
      "Wind:, 0.0161116063701896\n"
     ]
    }
   ],
   "source": [
    "play = [9, 5] # Yes, No\n",
    "\n",
    "# attribute, number of members (feature)\n",
    "outlook = [\n",
    "    [3, 1],  # overcast   [yes, no]\n",
    "    [6, 1],  # sunny      \n",
    "    [0, 3]   # rain\n",
    "]\n",
    "temperature = [\n",
    "    [1, 2],  # hot\n",
    "    [4, 2],  # cool\n",
    "    [4, 1]   # mild\n",
    "]\n",
    "humidity = [\n",
    "    [4, 3],  # high\n",
    "    [5, 2]   # normal\n",
    "]\n",
    "wind = [\n",
    "    [5, 2],  # no\n",
    "    [4, 3]   # yes\n",
    "]\n",
    "print (\"Information Gain:\\n\" )\n",
    "print(\"Outlook:\", IG(play, outlook))\n",
    "print(\"Temperature:\",IG(play, temperature))\n",
    "print(\"Humidity:\",IG(play, humidity))\n",
    "print(\"Wind:,\",IG(play, wind))\n",
    "# Information Gain:\n",
    "\n",
    "# Outlook: 0.41265581953400066\n",
    "# Temperature: 0.09212146003297261\n",
    "# Humidity: 0.0161116063701896\n",
    "# Wind:, 0.0161116063701896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here that the outlook attribute gives us the highest value for information gain, hence we choose this for creating a split at root node. So far we have our root node looking as below:\n",
    "![](images/tree-v1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Iteration\n",
    "\n",
    "Since the first iteration determines what split we should make for the root node of our tree, it's pretty simple. Now, we move down to the second level, and start finding the optimal split for each of the nodes on this level. The first branch (edge) of three above that leads to the \"Sunny\" outcome. Check for temperature, humidity and wind attributes to see which one provides the highest information gain.\n",
    "\n",
    "For the steps as above. Remember, we have 6 positive and 1 negative examples in the \"sunny\" branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain:\n",
      "\n",
      "Temperature: 0.7974288158134881\n",
      "Humidity: 0.9402859586706309\n",
      "Wind:, 0.5117145300992023\n"
     ]
    }
   ],
   "source": [
    "Play = [6, 1] \n",
    "\n",
    "temperature = [[1, 1],[3, 0], [2, 0]]  # hot, mild, cool [yes, no]  \n",
    "humidity = [[2, 0],[4, 1]]   # high, normal [yes, no]\n",
    "wind = [[3, 1],[3, 0]]      # Y, N [yes, no]\n",
    "\n",
    "\n",
    "print (\"Information Gain:\\n\" )\n",
    "\n",
    "print(\"Temperature:\",IG(play, temperature))\n",
    "print(\"Humidity:\",IG(play, humidity))\n",
    "print(\"Wind:,\",IG(play, wind))\n",
    "\n",
    "# Information Gain:\n",
    "\n",
    "# Temperature: 0.7974288158134881\n",
    "# Humidity: 0.6824544962108586\n",
    "# Wind:, 0.7084922088251644"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So here we see that temperature gives us the the highest information gain, so we'll use it to split our tree as shown below:\n",
    "![](images/humid.png)\n",
    "\n",
    "Let's continue. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Third Iteration\n",
    "\n",
    "We'll now calculate splits for the 'temperature' node we just created for days where the weather is sunny. Temperature has three possible values: [Hot, Mild, Cool]. This means that for each of the possible temperatures, we'll need to calculate if spliting on windy or humidity gives us the greatest possible information gain.\n",
    "\n",
    "Why are we doing this next instead of the rest of the splits on level 2? Because Decision Trees are a Greedy Algorithm, meaning that the next choice is always the one that will give it the greatest information gain. In this case, evaluating the temperature on sunny days gives us the most information gain, so that's where we'll go next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the Other Iterations\n",
    "\n",
    "What happens once we get down to a 'pure' split? Obviously, we stop splitting. Once that happens, we go back to the highest remaining uncalculated node, and calculate the best possible split for that one. We then continue on with that branch, until we have exhausted all possible splits or we run into a split that gives us 'pure' leaves where all 'play=Yes' is on one side of the split, and all 'play=No' is on the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you've seen:\n",
    "\n",
    "How to calculate entropy\n",
    "How to calculate information gain\n",
    "How to figure out the optimal split\n",
    "How to figure out what the next split you should calculate should be ('greedy' approach)\n",
    "This lab should have helped you familiarize yourself with how Decision Trees work 'under the hood', and demystified how the algorithm actually 'learns' from data. Great job!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
